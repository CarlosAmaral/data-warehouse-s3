# Data Warehouses with AWS's S3

## Introduction

This project aims to demonstrate the power of data warehouses with AWS S3's Redshift (based on Postgres).

The scripts available within the project allow an user to do an ETL process starting by creating all the necessary tables of types: fact, dimensional and staging using 2 datasets: the [Million Song Dataset](http://millionsongdataset.com/) and the Log Dataset -- log files generated by an event simulator based on the Million Song Dataset.



## Requirements
1) Install python 3.x.x;
2) Create an AWS S3's Redshift cluster;
3) Create an IAM role for the cluster;
4) Run `create_tables.py` to create all the staging, fact and dimensional tables that we need;
5) Run `sql_queries.py` to load all the data sets to the staging tables and later afterwards query the staging tables to populate the facts and the dimensional tables